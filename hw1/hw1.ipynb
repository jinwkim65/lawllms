{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "# Function to read corpus.txt from data folder\n",
    "def load_corpus_from_folder(folder_path):\n",
    "    corpus = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # Ensure only text files are read\n",
    "            with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                corpus.append(f.read())  # Read entire file as a single string\n",
    "    return corpus\n",
    "\n",
    "# Load text files from data folder into corpus\n",
    "# This assumes that the folder contains a subfolder \"data\" with the legal and literary documents in .txt format\n",
    "corpus_folder = \"data/\" \n",
    "corpus = load_corpus_from_folder(corpus_folder)\n",
    "\n",
    "# Pre-tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'United': 50, 'States': 50, 'Court': 51, 'of': 1672, 'Appeals': 3, 'FOR': 4, 'THE': 10, 'DISTRICT': 1, 'OF': 14, 'COLUMBIA': 1}\n"
     ]
    }
   ],
   "source": [
    "# Compute frequencies of each word in corpus as we do pre-tokenization\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "# Display frequencies of the first 10 words\n",
    "print(dict(list(word_freqs.items())[:10]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '##0',\n",
       " '##1',\n",
       " '##2',\n",
       " '##3',\n",
       " '##4',\n",
       " '##5',\n",
       " '##6',\n",
       " '##7',\n",
       " '##8',\n",
       " '##9',\n",
       " '##A',\n",
       " '##B',\n",
       " '##C',\n",
       " '##D',\n",
       " '##E',\n",
       " '##F',\n",
       " '##G',\n",
       " '##H',\n",
       " '##I',\n",
       " '##J',\n",
       " '##K',\n",
       " '##L',\n",
       " '##M',\n",
       " '##N',\n",
       " '##O',\n",
       " '##P',\n",
       " '##Q',\n",
       " '##R',\n",
       " '##S',\n",
       " '##T',\n",
       " '##U',\n",
       " '##V',\n",
       " '##W',\n",
       " '##X',\n",
       " '##Y',\n",
       " '##Z',\n",
       " '##a',\n",
       " '##b',\n",
       " '##c',\n",
       " '##d',\n",
       " '##e',\n",
       " '##f',\n",
       " '##g',\n",
       " '##h',\n",
       " '##i',\n",
       " '##j',\n",
       " '##k',\n",
       " '##l',\n",
       " '##m',\n",
       " '##n',\n",
       " '##o',\n",
       " '##p',\n",
       " '##q',\n",
       " '##r',\n",
       " '##s',\n",
       " '##t',\n",
       " '##u',\n",
       " '##v',\n",
       " '##w',\n",
       " '##x',\n",
       " '##y',\n",
       " '##z',\n",
       " '##™',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '>',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'y',\n",
       " 'z',\n",
       " '—',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '•',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate alphabet\n",
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('U', '##n'): 2.6814356894214924e-05\n",
      "('##n', '##i'): 1.7108658390789986e-06\n",
      "('##i', '##t'): 8.559704109335822e-06\n",
      "('##t', '##e'): 3.877434238834249e-06\n",
      "('##e', '##d'): 8.24734434073905e-06\n",
      "('S', '##t'): 5.827623951027689e-06\n"
     ]
    }
   ],
   "source": [
    "# Special tokens used by the model (BERT)\n",
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
    "\n",
    "# Split each word not prefixed by '##'\n",
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n",
    "\n",
    "# Function to computes the score of each pair\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# Compute pair scores on our corpus\n",
    "pair_scores = compute_pair_scores(splits)\n",
    "\n",
    "# Display the top 5 pair scores\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '!', '\"', '#', '##0', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##A', '##B', '##C', '##D', '##E', '##F', '##G', '##H', '##I', '##J', '##K', '##L', '##M', '##N', '##O', '##P', '##Q', '##R', '##S', '##T', '##U', '##V', '##W', '##X', '##Y', '##Z', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '##™', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', '—', '‘', '’', '“', '”', '•', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "# Function to merge a pair of tokens\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "# Merge to a fixed vocabulary size (70)\n",
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)\n",
    "\n",
    "# Display the final vocabulary\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode a word into tokens from vocabulary\n",
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "\n",
    "# Function to tokenize text\n",
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize legal document\n",
    "with open(\"data/legal_document.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    legal_text = f.read()\n",
    "\n",
    "legal_tokens = tokenize(legal_text)\n",
    "\n",
    "# Tokenize literary work\n",
    "with open(\"data/lit_work.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lit_work = f.read()\n",
    "\n",
    "lit_tokens = tokenize(lit_work)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the legal document: 131\n",
      "Number of unique tokens in the literary document: 148\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of unique tokens in each document\n",
    "print(f\"Number of unique tokens in the legal document: {len(set(legal_tokens))}\")\n",
    "print(f\"Number of unique tokens in the literary document: {len(set(lit_tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the legal document: 131\n",
      "Number of unique tokens in the literary document: 148\n",
      "Number of common tokens: 126\n",
      "Common tokens: {'##L', '##z', 'B', 'm', '7', 's', 'j', 'O', '##g', 'C', 'u', '##S', 'J', '-', 'l', '&', 'T', '.', '##k', '##C', '##a', '##J', 'N', '3', '(', 'z', '8', '##7', '##D', 'e', '##R', 'b', '##8', '##1', '##2', 'E', 'D', 'f', '##y', '##E', '##v', '##B', '##l', '##p', 'P', '##o', '6', ',', '##j', '##x', 'k', '##H', '##3', 'y', '##A', '?', 'i', 'U', '##4', '##I', 'L', '##t', '##u', '0', 'I', 'V', 'w', 'q', 'o', '##U', ')', '##r', '##b', '4', '[', '##e', 'Y', 'M', 'R', 'H', '##w', ';', 'c', 'h', '##0', 'g', 't', 'r', '##c', '##5', '##6', 'p', '##F', '##h', 'W', 'd', '##d', '1', '##i', '2', '9', '##m', 'a', '##O', 'Q', '##V', '##9', '##M', '/', ':', '##P', '##T', 'F', 'G', 'K', 'n', '5', '##s', '##q', '##n', ']', '##f', '*', 'v', 'S', 'A'}\n",
      "Number of unique tokens in the legal document: 5\n",
      "Number of unique tokens in the literary document: 22\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to sets for faster computation\n",
    "legal_tokens = set(legal_tokens)\n",
    "lit_tokens = set(lit_tokens)\n",
    "\n",
    "# Compute the number of unique tokens in each document\n",
    "print(f\"Number of unique tokens in the legal document: {len(legal_tokens)}\")\n",
    "print(f\"Number of unique tokens in the literary document: {len(lit_tokens)}\")\n",
    "\n",
    "# Compute intersection (common tokens)\n",
    "common_tokens = set(legal_tokens) & set(lit_tokens)\n",
    "print(f\"Number of common tokens: {len(common_tokens)}\")\n",
    "print(f\"Common tokens: {common_tokens}\")\n",
    "\n",
    "# Compute the number of tokens unique to each document\n",
    "unique_legal_tokens = legal_tokens - lit_tokens\n",
    "unique_lit_tokens = lit_tokens - legal_tokens\n",
    "print(f\"Number of unique tokens in the legal document: {len(unique_legal_tokens)}\")\n",
    "print(f\"Number of unique tokens in the literary document: {len(unique_lit_tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lawllms-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
